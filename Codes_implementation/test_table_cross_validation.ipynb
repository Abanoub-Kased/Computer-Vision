{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IXS6ZA2dVbZB"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import confusion_matrix,  ConfusionMatrixDisplay\n",
        "import os\n",
        "import shutil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "re3gTyrWb2di"
      },
      "outputs": [],
      "source": [
        "def test_model(path_to_model,path_to_data,output_file,model_name,data_name):\n",
        "  BATCH_SIZE = 60\n",
        "  IMG_SIZE = (224, 224)\n",
        "  model = tf.keras.models.load_model(path_to_model)\n",
        "  val_data = tf.keras.utils.image_dataset_from_directory(path_to_data,\n",
        "                                                            batch_size=BATCH_SIZE,\n",
        "                                                            image_size=IMG_SIZE,\n",
        "                                                            shuffle=True)\n",
        "  val_images =[]\n",
        "  true_labels = []\n",
        "  for image, label in val_data.unbatch():\n",
        "      val_images.append(image)\n",
        "      true_labels.append(label)\n",
        "  val_images = np.array(val_images)\n",
        "  true_labels = np.array(true_labels)\n",
        "  \n",
        "  loss, acc = model.evaluate(val_images, true_labels,verbose=0)\n",
        "\n",
        "  file=open(output_file,'a')\n",
        "  file.write('model name : {0} ,data_name : {1} '.format(model_name,data_name))\n",
        "  file.write(\"Accuracy: {:5.2f}% \\n\".format(100 * acc))\n",
        "  file.close()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlkHZR_kXIp0",
        "outputId": "98e4fcee-efab-4205-dceb-4a27e03ad31a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test model porto-ABC on set porto-D\n",
            "Found 500 files belonging to 4 classes.\n",
            "test model porto-ABC on dataset T_drive\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model porto-ABC on dataset san_fransisco\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model porto-ABD on set porto-C\n",
            "Found 500 files belonging to 4 classes.\n",
            "test model porto-ABD on dataset T_drive\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model porto-ABD on dataset san_fransisco\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model porto-ACD on set porto-B\n",
            "Found 500 files belonging to 4 classes.\n",
            "test model porto-ACD on dataset T_drive\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model porto-ACD on dataset san_fransisco\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model porto-BCD on set porto-A\n",
            "Found 500 files belonging to 4 classes.\n",
            "test model porto-BCD on dataset T_drive\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model porto-BCD on dataset san_fransisco\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model san_fransisco-ABC on set san_fransisco-D\n",
            "Found 500 files belonging to 4 classes.\n",
            "test model san_fransisco-ABC on dataset porto\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model san_fransisco-ABC on dataset T_drive\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model san_fransisco-ABD on set san_fransisco-C\n",
            "Found 500 files belonging to 4 classes.\n",
            "test model san_fransisco-ABD on dataset porto\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model san_fransisco-ABD on dataset T_drive\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model san_fransisco-ACD on set san_fransisco-B\n",
            "Found 500 files belonging to 4 classes.\n",
            "test model san_fransisco-ACD on dataset porto\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model san_fransisco-ACD on dataset T_drive\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model san_fransisco-BCD on set san_fransisco-A\n",
            "Found 500 files belonging to 4 classes.\n",
            "test model san_fransisco-BCD on dataset porto\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model san_fransisco-BCD on dataset T_drive\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model T_drive-ABC on set T_drive-D\n",
            "Found 500 files belonging to 4 classes.\n",
            "test model T_drive-ABC on dataset porto\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model T_drive-ABC on dataset san_fransisco\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model T_drive-ABD on set T_drive-C\n",
            "Found 500 files belonging to 4 classes.\n",
            "test model T_drive-ABD on dataset porto\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model T_drive-ABD on dataset san_fransisco\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model T_drive-ACD on set T_drive-B\n",
            "Found 500 files belonging to 4 classes.\n",
            "test model T_drive-ACD on dataset porto\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model T_drive-ACD on dataset san_fransisco\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model T_drive-BCD on set T_drive-A\n",
            "Found 500 files belonging to 4 classes.\n",
            "test model T_drive-BCD on dataset porto\n",
            "Found 400 files belonging to 4 classes.\n",
            "test model T_drive-BCD on dataset san_fransisco\n",
            "Found 400 files belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "path_to_models='test_table/models/'\n",
        "path_to_test_sets='test_table/test_sets/'\n",
        "path_to_real_data='test_table/real_data/'\n",
        "output_file='acc.txt'\n",
        "############################################################################################\n",
        "for model in os.listdir(path_to_models):\n",
        "  data_sets=['porto', 'T_drive', 'san_fransisco']\n",
        "  model_sets=os.listdir(path_to_models+model)\n",
        "  for model_set in model_sets:\n",
        "    path=path_to_models+model+'/'+model_set\n",
        "    path_to_model=path+'/fine_tune_model.h5' \n",
        "    if 'A' not in model_set:\n",
        "      path_to_data=path_to_test_sets+model+'/A'  \n",
        "      data_name=model+'-'+'A'\n",
        "    if 'B' not in model_set:\n",
        "       path_to_data=path_to_test_sets+model+'/B'\n",
        "       data_name=model+'-'+'B'\n",
        "    if 'C' not in model_set:\n",
        "       path_to_data=path_to_test_sets+model+'/C'\n",
        "       data_name=model+'-'+'C'\n",
        "    if 'D' not in model_set:\n",
        "       path_to_data=path_to_test_sets+model+'/D'\n",
        "       data_name=model+'-'+'D'\n",
        "    \n",
        "    \n",
        "    model_name=model+'-'+model_set\n",
        "    \n",
        "    \n",
        "    print(\"test model {0} on set {1}\".format(model_name,data_name))\n",
        "    #test on the set\n",
        "    test_model(path_to_model,path_to_data,output_file,model_name,data_name) \n",
        "    #test on different data set\n",
        "    other_datasets=data_sets\n",
        "    if model in other_datasets:\n",
        "      other_datasets.remove(model)\n",
        "    for dataset in other_datasets:\n",
        "      print('test model {0} on dataset {1}'.format(model_name,dataset))\n",
        "      path_to_data=path_to_real_data+'/'+dataset\n",
        "      data_name=dataset\n",
        "      \n",
        "      test_model(path_to_model,path_to_data,output_file,model_name,data_name) \n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSavz68yMEAw"
      },
      "outputs": [],
      "source": [
        "!cp acc.txt drive/MyDrive/project_codes/CLASSIFIERS/efficient_classifier/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
